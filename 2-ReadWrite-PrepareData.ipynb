{"cells":[{"cell_type":"markdown","id":"53477fb5","metadata":{},"source":["## Big Data Platforms Group project - GCP Read-Write data \n"]},{"cell_type":"markdown","id":"3f55bf84","metadata":{},"source":["### This notebook reads in all the data needed for the model, transform it and creates the \n","### file called Taxi_Uber_lyft_w_e_c_sdf that includes all the informacion"]},{"cell_type":"code","execution_count":1,"id":"8b4c88c8","metadata":{},"outputs":[],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql import Row\n","from pyspark.sql.types import *\n","sc = spark.sparkContext\n","from pyspark.sql import functions as F\n","from pyspark.sql.functions import *\n","from pyspark.sql.functions import col\n","from pyspark.sql.functions import regexp_replace\n","import pandas as pd\n","\n","spark = SparkSession.builder.appName('BDP-GroupProject').getOrCreate()\n","\n","def read_data(path):\n","    table = spark.read \\\n","    .option(\"quote\", \"\\\"\")  \\\n","    .option(\"escape\", \"\\\"\") \\\n","    .option(\"ignoreLeadingWhiteSpace\",True) \\\n","    .option(\"multiline\", True)\\\n","    .csv(path,inferSchema=True, header=True )\n","    return table"]},{"cell_type":"markdown","id":"6672bad2","metadata":{},"source":["### Taxi and Rideshare Data\n","- Variables unique to taxi data: 'taxi_id','tolls','extras','payment_type','company'\n","- Variables unique to rideshare data: 'additional_charges','trips_pooled','shared_trip_authorized'"]},{"cell_type":"code","execution_count":2,"id":"b16e1087","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["['trip_id', 'trip_start_timestamp', 'trip_end_timestamp', 'trip_seconds', 'trip_miles', 'pickup_census_tract', 'dropoff_census_tract', 'pickup_community_area', 'dropoff_community_area', 'fare', 'tips', 'additional_charges', 'trip_total', 'shared_trip_authorized', 'trips_pooled', 'pickup_centroid_latitude', 'pickup_centroid_longitude', 'pickup_centroid_location', 'dropoff_centroid_latitude', 'dropoff_centroid_longitude', 'dropoff_centroid_location', 'ride_type', 'payment_type']\n"]}],"source":["#----------\n","# Load rideshare parquet file\n","#----------\n","Uber_lyft_sdf = spark.read.parquet('gs://big-data-final/clean-parquet/rideshare.parquet')\n","\n","# correct column taxonomy structure\n","for col in Uber_lyft_sdf.columns:    \n","    Uber_lyft_sdf = Uber_lyft_sdf.withColumnRenamed(col, col.lower().replace(' ','_'))\n","    \n","# create additional columns:\n","Uber_lyft_sdf = Uber_lyft_sdf.withColumn('ride_type',F.lit('rideshare'))\n","Uber_lyft_sdf = Uber_lyft_sdf.withColumn('payment_type',F.lit('Mobile'))\n","Uber_lyft_sdf = Uber_lyft_sdf.withColumnRenamed('tip','tips')\n","#Uber_lyft_sdf = Uber_lyft_sdf.drop('trips_pooled','shared_trip_authorized')\n","\n","print(Uber_lyft_sdf.columns)"]},{"cell_type":"markdown","id":"4d6d78fd","metadata":{},"source":["\n","Taxi_sdf = {}\n","for years in [2019,2020,2021]:\n","    Taxi_sdf[years] = read_data(f\"/user/josefinabollini/GroupProject/Taxi_Trips_-_{years}.csv\")\n","  \n","Taxi_sdf = Taxi_sdf[2019].union(Taxi_sdf[2020]).union(Taxi_sdf[2021])\n"]},{"cell_type":"code","execution_count":3,"id":"7736f368","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['trip_start_timestamp', 'trip_end_timestamp', 'trip_seconds', 'trip_miles', 'pickup_community_area', 'dropoff_community_area', 'fare', 'tips', 'trip_total', 'payment_type', 'additional_charges', 'ride_type', 'trips_pooled', 'shared_trip_authorized']\n"]}],"source":["#----------\n","# Load taxi parquet file\n","#----------\n","Taxi_sdf = spark.read.parquet('gs://big-data-final/clean-parquet/taxi.parquet')\n","#print(Taxi_sdf.columns)\n","\n","#Taxi_sdf = Taxi_sdf.drop('Company','Taxi_ID')\n","\n","# 'additional_charges' = 'Tolls' + 'Extras'\n","from pyspark.sql.functions import col\n","Taxi_sdf = Taxi_sdf.withColumn('additional_charges',(col('Tolls')+col('Extras')))\n","\n","#Taxi_sdf = Taxi_sdf.withColumnRenamed('dropoff_centroid__location','dropoff_centroid_location')\n","\n","# drop variables:\n","#Taxi_sdf = Taxi_sdf.drop('Company','Taxi_ID', 'Trip_ID',\n","#                         'Tolls','Extras',\n","#                         'Pickup_Census_Tract','Dropoff_Census_Tract',\n","#                         'Pickup_Centroid_Latitude','Pickup_Centroid_Longitude','Pickup_Centroid_Location',\n","#                         'Dropoff_Centroid_Latitude','Dropoff_Centroid_Longitude','Dropoff_Centroid__Location')\n","\n","Taxi_sdf = Taxi_sdf.drop('Company','Taxi_ID', 'Trip_ID',\n","                         'Tolls','Extras',\n","                         'Pickup_Census_Tract','Dropoff_Census_Tract',\n","                         'Pickup_Centroid_Latitude','Pickup_Centroid_Longitude','Pickup_Centroid_Location',\n","                         'Dropoff_Centroid_Latitude','Dropoff_Centroid_Longitude','Dropoff_Centroid__Location')\n","\n","# correct column taxonomy structure\n","for col in Taxi_sdf.columns:\n","    Taxi_sdf = Taxi_sdf.withColumnRenamed(col, col.lower().replace(' ','_'))\n","\n","# create 'trips_pooled' and 'shared_trip_authorized' in Taxi_sdf\n","Taxi_sdf = Taxi_sdf.withColumn('ride_type',F.lit('taxi'))\n","Taxi_sdf = Taxi_sdf.withColumn('trips_pooled',F.lit(1))\n","Taxi_sdf = Taxi_sdf.withColumn('shared_trip_authorized',F.lit('False'))\n","Taxi_sdf = Taxi_sdf.withColumn('shared_trip_authorized',F.col('shared_trip_authorized').cast(BooleanType()))\n","\n","print(Taxi_sdf.columns)   "]},{"cell_type":"code","execution_count":4,"id":"6898aad7","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 2:>                                                          (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+--------------------+-------------------+------------+----------+---------------------+----------------------+----+----+----------+------------+------------------+---------+------------+----------------------+---------------+----------------+--------------+---------------+--------------+-------------+\n","|trip_start_timestamp|trip_end_timestamp |trip_seconds|trip_miles|pickup_community_area|dropoff_community_area|fare|tips|trip_total|payment_type|additional_charges|ride_type|trips_pooled|shared_trip_authorized|trip_start_date|trip_start_month|trip_start_day|trip_start_year|trip_start_dow|weekend_dummy|\n","+--------------------+-------------------+------------+----------+---------------------+----------------------+----+----+----------+------------+------------------+---------+------------+----------------------+---------------+----------------+--------------+---------------+--------------+-------------+\n","|2020-05-22 07:45:00 |2020-05-22 08:00:00|809         |1.4       |7                    |7                     |5.0 |0.0 |8.08      |Mobile      |3.08              |rideshare|1           |false                 |2020-05-22     |5               |22            |2020           |6             |1            |\n","|2020-05-08 17:15:00 |2020-05-08 17:30:00|923         |7.8       |32                   |3                     |12.5|4.0 |21.33     |Mobile      |4.83              |rideshare|1           |false                 |2020-05-08     |5               |8             |2020           |6             |1            |\n","+--------------------+-------------------+------------+----------+---------------------+----------------------+----+----+----------+------------+------------------+---------+------------+----------------------+---------------+----------------+--------------+---------------+--------------+-------------+\n","only showing top 2 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["#----------\n","# join taxi and rideshare data\n","#----------\n","col_list = list(Taxi_sdf.columns)\n","Uber_lyft_sdf = Uber_lyft_sdf.select(col_list) #select rideshare columns present in taxi df\n","Taxi_Uber_lyft_sdf = Uber_lyft_sdf.unionAll(Taxi_sdf)\n","\n","#-----\n","# format timestamp\n","#-----\n","Taxi_Uber_lyft_sdf = Taxi_Uber_lyft_sdf.withColumn('trip_start_timestamp', \n","                                                   F.from_unixtime(F.unix_timestamp('trip_start_timestamp',\n","                                                                                    'MM/dd/yyyy hh:mm:ss a'),\n","                                                                   'yyyy-MM-dd HH:mm:ss').cast('timestamp'))  \n","\n","Taxi_Uber_lyft_sdf = Taxi_Uber_lyft_sdf.withColumn('trip_end_timestamp', \n","                                                   F.from_unixtime(F.unix_timestamp('trip_end_timestamp',\n","                                                                                    'MM/dd/yyyy hh:mm:ss a'),\n","                                                                   'yyyy-MM-dd HH:mm:ss').cast('timestamp'))  \n","#-----\n","# extract date (trip start)\n","#-----\n","Taxi_Uber_lyft_sdf = Taxi_Uber_lyft_sdf.withColumn('trip_start_date', \n","                                                   F.from_unixtime(F.unix_timestamp('trip_start_timestamp',\n","                                                                                    'MM/dd/yyyy hh:mm:ss a'),\n","                                                                   'yyyy-MM-dd').cast('date'))  \n","#-----\n","# extract month, day, year, and dow from 'trip_start_date'\n","#-----\n","Taxi_Uber_lyft_sdf = Taxi_Uber_lyft_sdf.withColumn('trip_start_month',F.month('trip_start_date'))\n","Taxi_Uber_lyft_sdf = Taxi_Uber_lyft_sdf.withColumn('trip_start_day',F.dayofmonth('trip_start_date'))\n","Taxi_Uber_lyft_sdf = Taxi_Uber_lyft_sdf.withColumn('trip_start_year',F.year('trip_start_date'))\n","Taxi_Uber_lyft_sdf = Taxi_Uber_lyft_sdf.withColumn('trip_start_dow',F.dayofweek('trip_start_date'))\n","#Taxi_Uber_lyft_sdf = Taxi_Uber_lyft_sdf.withColumn('trip_end_month',F.month('trip_end_timestamp'))\n","#Taxi_Uber_lyft_sdf = Taxi_Uber_lyft_sdf.withColumn('trip_end_day',F.dayofmonth('trip_end_timestamp'))\n","#Taxi_Uber_lyft_sdf = Taxi_Uber_lyft_sdf.withColumn('trip_end_year',F.year('trip_end_timestamp'))\n","#Taxi_Uber_lyft_sdf = Taxi_Uber_lyft_sdf.withColumn('trip_end_dow',F.dayofweek('trip_end_timestamp'))\n","\n","#Taxi_Uber_lyft_sdf.select('start_day','end_month','start_year','start_dow').drop_duplicates().show(3)\n","\n","#-----\n","# add weekend vs weekday dummy\n","#-----\n","weekend_days = [1,6,7]\n","Taxi_Uber_lyft_sdf = Taxi_Uber_lyft_sdf.withColumn('weekend_dummy',\\\n","                                                   F.when(F.col('trip_start_dow').isin(weekend_days),1)\\\n","                                                   .otherwise(0))\n","\n","Taxi_Uber_lyft_sdf.show(2,truncate=False)"]},{"cell_type":"code","execution_count":5,"id":"6c162353","metadata":{},"outputs":[{"data":{"text/plain":["[('trip_start_timestamp', 'timestamp'),\n"," ('trip_end_timestamp', 'timestamp'),\n"," ('trip_seconds', 'int'),\n"," ('trip_miles', 'double'),\n"," ('pickup_community_area', 'int'),\n"," ('dropoff_community_area', 'int'),\n"," ('fare', 'double'),\n"," ('tips', 'double'),\n"," ('trip_total', 'double'),\n"," ('payment_type', 'string'),\n"," ('additional_charges', 'double'),\n"," ('ride_type', 'string'),\n"," ('trips_pooled', 'int'),\n"," ('shared_trip_authorized', 'boolean'),\n"," ('trip_start_date', 'date'),\n"," ('trip_start_month', 'int'),\n"," ('trip_start_day', 'int'),\n"," ('trip_start_year', 'int'),\n"," ('trip_start_dow', 'int'),\n"," ('weekend_dummy', 'int')]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["Taxi_Uber_lyft_sdf.dtypes"]},{"cell_type":"code","execution_count":6,"id":"4b40f4a1","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["462314865"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["#Taxi_Uber_lyft_sdf.count() #462314865 for all years"]},{"cell_type":"code","execution_count":7,"id":"adc58191","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+---------------+---------+\n","|trip_start_year|    count|\n","+---------------+---------+\n","|           2018| 38193541|\n","|           2015| 32385875|\n","|           2022| 49072066|\n","|           2013| 27217716|\n","|           null|        4|\n","|           2014| 37395436|\n","|           2019|125190465|\n","|           2020| 40998888|\n","|           2016| 31759339|\n","|           2017| 24988003|\n","|           2021| 55113532|\n","+---------------+---------+\n","\n"]}],"source":["#Taxi_Uber_lyft_sdf.groupBy('trip_start_year').count().show()"]},{"cell_type":"code","execution_count":6,"id":"70691985","metadata":{},"outputs":[],"source":["Taxi_Uber_lyft_sdf = Taxi_Uber_lyft_sdf.filter((Taxi_Uber_lyft_sdf.trip_start_year == 2019) | (Taxi_Uber_lyft_sdf.trip_start_year == 2020) | (Taxi_Uber_lyft_sdf.trip_start_year == 2021))"]},{"cell_type":"code","execution_count":9,"id":"41c83939","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 13:=====================================================>(213 + 1) / 214]\r"]},{"name":"stdout","output_type":"stream","text":["+---------------+---------+\n","|trip_start_year|    count|\n","+---------------+---------+\n","|           2019|125190465|\n","|           2020| 40998888|\n","|           2021| 55113532|\n","+---------------+---------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["#Taxi_Uber_lyft_sdf.groupBy('trip_start_year').count().show()"]},{"cell_type":"code","execution_count":24,"id":"5be7e3a7","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["221302885"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["#Taxi_Uber_lyft_sdf.count() #expect 250 mill, this is the actual number 221,302,885"]},{"cell_type":"markdown","id":"01719faf","metadata":{},"source":["### Weather Data\n","\n","***Station Names in weather data do NOT align with community areas. To solve this issue, create a \"chicagoland view\" by summing all station's values for each date.***\n","\n","NOTE: 9’s in a field (e.g.9999) indicate missing data or data that has not been received.\n","\n","The five core values are:\n","- PRCP = Precipitation (mm or inches as per user preference, inches to hundredths on Daily Form pdf file)\n","- SNOW = Snowfall (mm or inches as per user preference, inches to tenths on Daily Form pdf file)\n","- SNWD = Snow depth (mm or inches as per user preference, inches on Daily Form pdf file)\n","- TMAX = Maximum temperature (Fahrenheit or Celsius as per user preference, Fahrenheit to tenths on Daily Form pdf file\n","- TMIN = Minimum temperature (Fahrenheit or Celsius as per user preference, Fahrenheit to tenths on Daily Form pdf file"]},{"cell_type":"code","execution_count":7,"id":"20d2c32f","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/11/26 06:58:52 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #1,5,main]) interrupted: \n","java.lang.InterruptedException\n","\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n","\tat com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n","\tat org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n","\tat org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","[Stage 7:>                                                          (0 + 2) / 2]\r"]},{"name":"stdout","output_type":"stream","text":["+----------+------------------+----+\n","|      DATE|              PRCP|SNOW|\n","+----------+------------------+----+\n","|2021-06-22|              0.02| 0.0|\n","|2021-08-27|1.7200000000000002| 0.0|\n","+----------+------------------+----+\n","only showing top 2 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["#----------\n","# Load Weather csv files\n","#----------\n","W2021_sdf = read_data('gs://big-data-final/raw-weather/2021 weather data.csv')\n","W1920_sdf = read_data('gs://big-data-final/raw-weather/2019-2020 weather data.csv')\n","#print(W2021_sdf.columns)\n","#print(W1920_sdf.columns)\n","\n","from pyspark.sql.functions import col\n","\n","def align_weather_data(datain):\n","    # temprarture is null - only keeping snow and precipitation\n","    w_sdf = datain.select('NAME','DATE','PRCP','SNOW')        \n","    # date type\n","    w_sdf = w_sdf.withColumn('DATE', to_date(col('DATE'),'yyyy-MM-dd'))    \n","    return w_sdf\n","\n","Weather_all_sdf = align_weather_data(W2021_sdf).unionAll(align_weather_data(W1920_sdf)).fillna(0)\n","\n","#-----\n","# sum precipitation and snow by date, for all Chicago stations\n","#-----\n","Weather_all_sdf = Weather_all_sdf.groupBy(\"DATE\").sum(\"PRCP\",\"SNOW\")\n","Weather_all_sdf = Weather_all_sdf.withColumnRenamed(\"sum(PRCP)\",\"PRCP\")\n","Weather_all_sdf = Weather_all_sdf.withColumnRenamed(\"sum(SNOW)\",\"SNOW\")\n","\n","Weather_all_sdf.show(2)"]},{"cell_type":"code","execution_count":8,"id":"5c0fda1c","metadata":{},"outputs":[{"data":{"text/plain":["[('trip_start_timestamp', 'timestamp'),\n"," ('trip_end_timestamp', 'timestamp'),\n"," ('trip_seconds', 'int'),\n"," ('trip_miles', 'double'),\n"," ('pickup_community_area', 'int'),\n"," ('dropoff_community_area', 'int'),\n"," ('fare', 'double'),\n"," ('tips', 'double'),\n"," ('trip_total', 'double'),\n"," ('payment_type', 'string'),\n"," ('additional_charges', 'double'),\n"," ('ride_type', 'string'),\n"," ('trips_pooled', 'int'),\n"," ('shared_trip_authorized', 'boolean'),\n"," ('trip_start_date', 'date'),\n"," ('trip_start_month', 'int'),\n"," ('trip_start_day', 'int'),\n"," ('trip_start_year', 'int'),\n"," ('trip_start_dow', 'int'),\n"," ('weekend_dummy', 'int'),\n"," ('PRCP', 'double'),\n"," ('SNOW', 'double')]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["#----------\n","# merge weather and taxi/rideshare data\n","#----------\n","Taxi_Uber_lyft_sdf.createOrReplaceTempView(\"taxi\")\n","Weather_all_sdf.createOrReplaceTempView(\"weather\")\n","\n","# We are choosing to merge by start dates - as is when the user made the decision to order a ride\n","Taxi_Uber_lyft_w_sdf = spark.sql(\"\"\"SELECT t.*, w.*\n","                                    FROM \n","                                    taxi t left join weather w \n","                                    on t.trip_start_date = w.DATE \"\"\")\n","\n","# drop \"DATE\" column (from weather data)\n","Taxi_Uber_lyft_w_sdf = Taxi_Uber_lyft_w_sdf.drop('DATE')\n","\n","Taxi_Uber_lyft_w_sdf.dtypes"]},{"cell_type":"code","execution_count":null,"id":"098bc65c","metadata":{},"outputs":[],"source":["#Taxi_Uber_lyft_w_sdf.count()"]},{"cell_type":"markdown","id":"45b68224","metadata":{},"source":["### Events Data\n","\n","Events data includes Chicago sporting events.\n","\n","NOTE: `nb_code` is equivalent to `community_area_code`"]},{"cell_type":"code","execution_count":9,"id":"65f9a8d1","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+-----+----+--------------------+-----+\n","|DAY|MONTH|YEAR|event_community_area|count|\n","+---+-----+----+--------------------+-----+\n","| 19|    5|2019|                  34|    1|\n","| 27|    5|2019|                  34|    2|\n","| 29|    5|2021|                   6|    1|\n","| 20|   12|2021|                  33|    1|\n","|  2|    9|2021|                  34|    1|\n","+---+-----+----+--------------------+-----+\n","only showing top 5 rows\n","\n"]},{"data":{"text/plain":["[('DAY', 'int'),\n"," ('MONTH', 'int'),\n"," ('YEAR', 'int'),\n"," ('event_community_area', 'int'),\n"," ('count', 'bigint')]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["#----------\n","# Load Events csv file\n","#----------\n","All_events_data_sdf = read_data('gs://big-data-final/raw-events/All_events_aligned.csv')\n","All_events_data_sdf = All_events_data_sdf.withColumnRenamed('nb_code','event_community_area')\n","#All_events_data_sdf = All_events_data_sdf.withColumnRenamed('NEIGHBORHOOD','event_community_area_name')\n","\n","#-----\n","# count the number of events by date, in each community\n","#-----\n","All_events_data_sdf = All_events_data_sdf.groupby('DAY','MONTH','YEAR','event_community_area').count()\n","\n","All_events_data_sdf.show(5)\n","All_events_data_sdf.dtypes"]},{"cell_type":"code","execution_count":10,"id":"a8046166","metadata":{},"outputs":[],"source":["# Merge with Uber and Taxis data\n","Taxi_Uber_lyft_w_sdf.createOrReplaceTempView(\"taxi2\")\n","All_events_data_sdf.createOrReplaceTempView(\"events\")\n","\n","# merge by ride start date\n","\n","# 1. left join events data by date and PICKUP COMMUNITY\n","Taxi_Uber_lyft_w_e_sdf = spark.sql(\"\"\"SELECT t.*,\n","                                             e.*\n","                                      FROM taxi2 t left join events e \n","                                          on t.trip_start_day = e.DAY and \n","                                             t.trip_start_month = e.MONTH and\n","                                             t.trip_start_year = e.YEAR and\n","                                             t.pickup_community_area = e.event_community_area\"\"\")\n","\n","Taxi_Uber_lyft_w_e_sdf = Taxi_Uber_lyft_w_e_sdf.drop('DAY','MONTH','YEAR','event_community_area')\n","Taxi_Uber_lyft_w_e_sdf = Taxi_Uber_lyft_w_e_sdf.withColumnRenamed('count','pickup_community_eventCnt')\n","#Taxi_Uber_lyft_w_e_sdf.dtypes"]},{"cell_type":"code","execution_count":11,"id":"f0914b0a","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/11/26 06:59:00 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"]},{"data":{"text/plain":["[('trip_start_timestamp', 'timestamp'),\n"," ('trip_end_timestamp', 'timestamp'),\n"," ('trip_seconds', 'int'),\n"," ('trip_miles', 'double'),\n"," ('pickup_community_area', 'int'),\n"," ('dropoff_community_area', 'int'),\n"," ('fare', 'double'),\n"," ('tips', 'double'),\n"," ('trip_total', 'double'),\n"," ('payment_type', 'string'),\n"," ('additional_charges', 'double'),\n"," ('ride_type', 'string'),\n"," ('trips_pooled', 'int'),\n"," ('shared_trip_authorized', 'boolean'),\n"," ('trip_start_date', 'date'),\n"," ('trip_start_month', 'int'),\n"," ('trip_start_day', 'int'),\n"," ('trip_start_year', 'int'),\n"," ('trip_start_dow', 'int'),\n"," ('weekend_dummy', 'int'),\n"," ('PRCP', 'double'),\n"," ('SNOW', 'double'),\n"," ('pickup_community_eventCnt', 'bigint'),\n"," ('dropoff_community_eventCnt', 'bigint'),\n"," ('community_eventCnt', 'bigint')]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# 2. left join events data by date and DROPOFF COMMUNITY\n","Taxi_Uber_lyft_w_e_sdf.createOrReplaceTempView(\"taxi_e\")\n","\n","Taxi_Uber_lyft_w_e_sdf = spark.sql(\"\"\"SELECT t.*,\n","                                             e.*\n","                                      FROM taxi_e t left join events e \n","                                          on t.trip_start_day = e.DAY and \n","                                             t.trip_start_month = e.MONTH and\n","                                             t.trip_start_year = e.YEAR and\n","                                             t.dropoff_community_area = e.event_community_area\"\"\")\n","\n","Taxi_Uber_lyft_w_e_sdf = Taxi_Uber_lyft_w_e_sdf.drop('DAY','MONTH','YEAR','event_community_area')\n","Taxi_Uber_lyft_w_e_sdf = Taxi_Uber_lyft_w_e_sdf.withColumnRenamed('count','dropoff_community_eventCnt')\n","\n","Taxi_Uber_lyft_w_e_sdf = Taxi_Uber_lyft_w_e_sdf.fillna(0)\n","\n","#-----\n","# create a total count of the events occurring in a rider's pickup & dropoff community\n","# if the pickup and dropoff community are the same, do NOT double count events\n","#-----\n","Taxi_Uber_lyft_w_e_sdf = Taxi_Uber_lyft_w_e_sdf.withColumn('community_eventCnt',\n","                                                           F.when(F.col('pickup_community_area')==F.col('dropoff_community_area'), \n","                                                                  col('dropoff_community_eventCnt')\n","                                                                 ).otherwise(col('dropoff_community_eventCnt')+col('pickup_community_eventCnt')))\n","\n","Taxi_Uber_lyft_w_e_sdf.dtypes"]},{"cell_type":"markdown","id":"3647cc4a","metadata":{},"source":["Check transformation\n","```\n","Taxi_Uber_lyft_w_e_sdf.filter(Taxi_Uber_lyft_w_e_sdf.community_eventCnt>0\n","                             ).select('dropoff_community_area',\n","                                      'pickup_community_area',\n","                                      'pickup_community_eventCnt',\n","                                      'dropoff_community_eventCnt',\n","                                      'community_eventCnt').show(5)\n","\n","Taxi_Uber_lyft_w_e_sdf.filter((Taxi_Uber_lyft_w_e_sdf.community_eventCnt>0) &\n","                             (Taxi_Uber_lyft_w_e_sdf.dropoff_community_area==Taxi_Uber_lyft_w_e_sdf.pickup_community_area\n","                             )).select('dropoff_community_area',\n","                                       'pickup_community_area',               \n","                                       'pickup_community_eventCnt',\n","                                       'dropoff_community_eventCnt',\n","                                       'community_eventCnt').show(5)\n","```"]},{"cell_type":"markdown","id":"379d05fc","metadata":{},"source":["### Add Communities data "]},{"cell_type":"code","execution_count":12,"id":"26c55655","metadata":{},"outputs":[],"source":["#----------\n","# Load Community Names parquet file\n","#----------\n","chi_comunities_sdf = spark.read.parquet(\"gs://big-data-final/clean-parquet/chi_communities.parquet\")\n","#chi_comunities_sdf.show(5)\n","\n","#print(Taxi_Uber_lyft_w_e_sdf.columns)\n","\n","Taxi_Uber_lyft_w_e_sdf.createOrReplaceTempView(\"taxi\")\n","chi_comunities_sdf.createOrReplaceTempView(\"chicom\")\n","\n","# 1. merge pickup_community_name\n","Taxi_Uber_lyft_w_e_sdf = spark.sql(\"\"\"SELECT t.*,\n","                                        c.community_area_name as pickup_community_name\n","                                      FROM \n","                                        taxi t left join chicom c \n","                                        on t.pickup_community_area = c.community_area\"\"\")\n","\n","# 2. merge dropoff_community_name\n","Taxi_Uber_lyft_w_e_sdf.createOrReplaceTempView(\"taxi2\")\n","Taxi_Uber_lyft_w_e_sdf = spark.sql(\"\"\"SELECT t.*,\n","                                        c.community_area_name as dropoff_community_name\n","                                      FROM \n","                                        taxi2 t left join chicom c \n","                                        on t.dropoff_community_area = c.community_area\"\"\")"]},{"cell_type":"code","execution_count":13,"id":"b898b541","metadata":{},"outputs":[],"source":["# Dummy for rides that started or finished outside Chicagoland\n","def dummy_communities(dat,var_in,dummy_out):\n","    dat = \\\n","    dat.withColumn(dummy_out,F.when(F.col(var_in) == 0,0).otherwise(1))\n","    return dat\n","\n","Taxi_Uber_lyft_w_e_sdf = dummy_communities(Taxi_Uber_lyft_w_e_sdf,'pickup_community_area','chicago_pickup')\n","Taxi_Uber_lyft_w_e_sdf = dummy_communities(Taxi_Uber_lyft_w_e_sdf,'dropoff_community_area','chicago_dropoff')\n","\n","Taxi_Uber_lyft_w_e_sdf = Taxi_Uber_lyft_w_e_sdf.withColumn('outside_chicago_ride',\\\n","                             F.when((F.col('chicago_pickup') == 0) & (F.col('chicago_dropoff') == 0),0).\\\n","                                                               otherwise(1))"]},{"cell_type":"markdown","id":"dab4cce5","metadata":{},"source":["### Add Covid data"]},{"cell_type":"code","execution_count":14,"id":"ff8be657","metadata":{},"outputs":[],"source":["#----------\n","# Load Covid-19 csv file\n","#----------\n","covid_sdf = read_data('gs://big-data-final/raw-covid/covid.csv')\n","#covid_sdf.show(5)\n","covid_sdf = covid_sdf.select('Date','Cases - Total','Deaths - Total','Hospitalizations - Total')\n","\n","# rename columns\n","for col in ['Cases - Total','Deaths - Total','Hospitalizations - Total']:\n","    covid_sdf = covid_sdf.withColumnRenamed(col, col.lower().replace('-','').replace('  ','_'))\n","\n","covid_sdf = covid_sdf.withColumn('date_to_use',F.from_unixtime(F.unix_timestamp('Date','MM/dd/yyyy')\\\n","                                                                   ,'yyyy-MM-dd').cast('date'))\n","# drop Date string variable\n","covid_sdf = covid_sdf.drop(\"Date\")\n","\n","# sort by date_to_use, ascending\n","covid_sdf = covid_sdf.sort(\"date_to_use\")\n","\n","# create month, year, and day columns\n","covid_sdf1 = covid_sdf.withColumn('MONTH', F.month('date_to_use')\n","                                 ).withColumn('YEAR', F.year('date_to_use')\n","                                             ).withColumn('day',F.dayofmonth('date_to_use'))\n","\n","# drop rows with null value of \"date_to_use\" \n","covid_sdf1 = covid_sdf1.na.drop(subset=[\"date_to_use\"])\n","\n","#-----\n","# create rolling average of Covid-19 metrics\n","#-----\n","covid_sdf1.createOrReplaceTempView(\"covid1\")\n","covid_sdf2 = spark.sql(\"\"\"SELECT  *, \n","                              avg(`cases_total`) OVER(ORDER BY date_to_use\n","                              ROWS BETWEEN 6 PRECEDING AND CURRENT ROW)\n","                               as covid_cases_sma7,\n","                              avg(`hospitalizations_total`) OVER(ORDER BY date_to_use\n","                              ROWS BETWEEN 6 PRECEDING AND CURRENT ROW)\n","                               as covid_hosp_sma7,\n","                              avg(`deaths_total`) OVER(ORDER BY date_to_use\n","                              ROWS BETWEEN 6 PRECEDING AND CURRENT ROW)\n","                                as covid_deaths_sma7\n","                          FROM covid1 \"\"\")\n","\n","covid_sdf2 = covid_sdf2.fillna(0)"]},{"cell_type":"code","execution_count":15,"id":"a03b376d","metadata":{},"outputs":[{"data":{"text/plain":["[('trip_start_timestamp', 'timestamp'),\n"," ('trip_end_timestamp', 'timestamp'),\n"," ('trip_seconds', 'int'),\n"," ('trip_miles', 'double'),\n"," ('pickup_community_area', 'int'),\n"," ('dropoff_community_area', 'int'),\n"," ('fare', 'double'),\n"," ('tips', 'double'),\n"," ('trip_total', 'double'),\n"," ('payment_type', 'string'),\n"," ('additional_charges', 'double'),\n"," ('ride_type', 'string'),\n"," ('trips_pooled', 'int'),\n"," ('shared_trip_authorized', 'boolean'),\n"," ('trip_start_date', 'date'),\n"," ('trip_start_month', 'int'),\n"," ('trip_start_day', 'int'),\n"," ('trip_start_year', 'int'),\n"," ('trip_start_dow', 'int'),\n"," ('weekend_dummy', 'int'),\n"," ('PRCP', 'double'),\n"," ('SNOW', 'double'),\n"," ('pickup_community_eventCnt', 'bigint'),\n"," ('dropoff_community_eventCnt', 'bigint'),\n"," ('community_eventCnt', 'bigint'),\n"," ('pickup_community_name', 'string'),\n"," ('dropoff_community_name', 'string'),\n"," ('chicago_pickup', 'int'),\n"," ('chicago_dropoff', 'int'),\n"," ('outside_chicago_ride', 'int'),\n"," ('covid_cases_sma7', 'double'),\n"," ('covid_hosp_sma7', 'double'),\n"," ('covid_deaths_sma7', 'double')]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# merge data\n","Taxi_Uber_lyft_w_e_sdf.createOrReplaceTempView(\"taxi2\")\n","covid_sdf2.createOrReplaceTempView(\"covid2\")\n","\n","Taxi_Uber_lyft_w_e_c_sdf = spark.sql(\"\"\"SELECT t.*,c.*\n","                                        FROM \n","                                        taxi2 t left join covid2 c \n","                                        on t.trip_start_date = c.date_to_use \"\"\")\n","\n","Taxi_Uber_lyft_w_e_c_sdf = Taxi_Uber_lyft_w_e_c_sdf.drop('day','MONTH','YEAR','date_to_use',\n","                                                         'cases_total','deaths_total','hospitalizations_total')\n","\n","Taxi_Uber_lyft_w_e_c_sdf.dtypes"]},{"cell_type":"code","execution_count":null,"id":"170a5ede","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["221302885"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["#Taxi_Uber_lyft_w_e_c_sdf.count() #221,302,885"]},{"cell_type":"code","execution_count":16,"id":"91f415f1","metadata":{},"outputs":[],"source":["Taxi_Uber_lyft_w_e_c_sdf = Taxi_Uber_lyft_w_e_c_sdf.na.fill(value=0)"]},{"cell_type":"code","execution_count":null,"id":"7fa18512","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/11/26 05:42:41 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n","22/11/26 05:42:44 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n","22/11/26 05:42:44 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n","22/11/26 05:42:44 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n","22/11/26 05:42:44 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n","22/11/26 05:42:44 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n","22/11/26 05:42:45 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n","22/11/26 05:42:45 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["done\n"]}],"source":["#Taxi_Uber_lyft_w_e_c_sdf.write.parquet('gs://big-data-final/model-data/final-model2.parquet')\n","#print('done')"]},{"cell_type":"markdown","id":"a951ddc2","metadata":{},"source":["# Machine Learning Models"]},{"cell_type":"markdown","id":"190e6be4","metadata":{},"source":["## Feature Engineering"]},{"cell_type":"code","execution_count":17,"id":"66fb4469","metadata":{},"outputs":[],"source":["from pyspark.ml import Pipeline\n","from pyspark.ml.classification import NaiveBayes\n","from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml.feature import StringIndexer,IndexToString,VectorAssembler,OneHotEncoder"]},{"cell_type":"code","execution_count":18,"id":"a2db5332","metadata":{},"outputs":[{"data":{"text/plain":["[('trip_start_timestamp', 'timestamp'),\n"," ('trip_seconds', 'int'),\n"," ('trip_miles', 'double'),\n"," ('fare', 'double'),\n"," ('tips', 'double'),\n"," ('trip_total', 'double'),\n"," ('payment_type', 'string'),\n"," ('additional_charges', 'double'),\n"," ('ride_type', 'int'),\n"," ('trips_pooled', 'int'),\n"," ('shared_trip_authorized', 'int'),\n"," ('trip_start_date', 'date'),\n"," ('trip_start_month', 'int'),\n"," ('trip_start_day', 'int'),\n"," ('trip_start_year', 'int'),\n"," ('trip_start_dow', 'int'),\n"," ('weekend_dummy', 'int'),\n"," ('community_eventCnt', 'bigint'),\n"," ('pickup_community_name', 'string'),\n"," ('dropoff_community_name', 'string'),\n"," ('outside_chicago_ride', 'int'),\n"," ('covid_cases_sma7', 'double'),\n"," ('covid_hosp_sma7', 'double'),\n"," ('covid_deaths_sma7', 'double'),\n"," ('label', 'int'),\n"," ('fare_add', 'double'),\n"," ('tip_pct', 'double'),\n"," ('add_charge_pct', 'double'),\n"," ('hour', 'int'),\n"," ('rain_snow', 'int'),\n"," ('winter', 'int'),\n"," ('spring', 'int'),\n"," ('summer', 'int'),\n"," ('autumn', 'int'),\n"," ('sunday', 'int'),\n"," ('monday', 'int'),\n"," ('tuesday', 'int'),\n"," ('wednesday', 'int'),\n"," ('thursday', 'int'),\n"," ('friday', 'int'),\n"," ('saturday', 'int')]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# filter for rides with non-zero fare\n","modData = Taxi_Uber_lyft_w_e_c_sdf.filter(F.col('fare')>0)\n","\n","# define 'label'\n","#    1: rider leaves a tip\n","#    0: rider does not leave a tip\n","modData = modData.withColumn(\"label\",\n","                             F.when((F.col('tips')>0),1).otherwise(0))\n","\n","# convert 'ride_type' to integer\n","#    1: rideshare\n","#    0: taxi\n","modData = modData.withColumn(\"ride_type\",\n","                             F.when((F.col('ride_type')==\"rideshare\"),1).otherwise(0).cast('integer'))\n","\n","# define 'fare_add' as fare + additional charges (*note, trip_total includes tip)\n","modData = modData.withColumn(\"fare_add\",\n","                             (F.col('fare')+F.col('additional_charges')))\n","\n","# define 'tip_pct' as the tip dollar value / (fare + additional_charges)\n","modData = modData.withColumn(\"tip_pct\",\n","                             (F.col('tips') / F.col('fare_add')))\n","\n","# define 'add_charge_pct' as the percent of the pre-tip ride cost coming from additional_charges\n","# (additional_charges / (fare + additional_charges))\n","modData = modData.withColumn(\"add_charge_pct\",\n","                             (F.col('additional_charges') / F.col('fare_add')))\n","\n","# add hour column\n","modData = modData.withColumn('hour', hour(modData.trip_start_timestamp))\n","\n","# add rain_snow column\n","modData = modData.withColumn(\"rain_snow\",\n","                             F.when((F.col('PRCP')+F.col('SNOW'))>0, 1).otherwise(0))\n","\n","# add 'winter' column\n","modData = modData.withColumn(\"winter\",\n","                             F.when(((F.col('trip_start_month')==1) | \n","                                     (F.col('trip_start_month')==2) | \n","                                     (F.col('trip_start_month')==3)), 1).otherwise(0))\n","# add 'spring' column\n","modData = modData.withColumn(\"spring\",\n","                             F.when(((F.col('trip_start_month')==4) | \n","                                     (F.col('trip_start_month')==5) | \n","                                     (F.col('trip_start_month')==6)), 1).otherwise(0))\n","# add 'summer' column\n","modData = modData.withColumn(\"summer\",\n","                             F.when(((F.col('trip_start_month')==7) | \n","                                     (F.col('trip_start_month')==8) | \n","                                     (F.col('trip_start_month')==9)), 1).otherwise(0))\n","# add 'autumn' column\n","modData = modData.withColumn(\"autumn\",\n","                             F.when(((F.col('trip_start_month')==10) | \n","                                     (F.col('trip_start_month')==11) | \n","                                     (F.col('trip_start_month')==12)), 1).otherwise(0))\n","# create dow columns, Monday=0, Sunday=6\n","modData = modData.withColumn(\"sunday\",F.when(F.col('trip_start_dow')==1,1).otherwise(0))\n","modData = modData.withColumn(\"monday\",F.when(F.col('trip_start_dow')==2,1).otherwise(0))\n","modData = modData.withColumn(\"tuesday\",F.when(F.col('trip_start_dow')==3,1).otherwise(0))\n","modData = modData.withColumn(\"wednesday\",F.when(F.col('trip_start_dow')==4,1).otherwise(0))\n","modData = modData.withColumn(\"thursday\",F.when(F.col('trip_start_dow')==5,1).otherwise(0))\n","modData = modData.withColumn(\"friday\",F.when(F.col('trip_start_dow')==6,1).otherwise(0))\n","modData = modData.withColumn(\"saturday\",F.when(F.col('trip_start_dow')==7,1).otherwise(0))\n","\n","# convert 'shared_trip_authorized' from boolean to integer\n","modData = modData.withColumn(\"shared_trip_authorized\",\n","                             when(F.col('shared_trip_authorized')==True,1).otherwise(0).cast('integer'))\n","\n","# convert categorical features to string type\n","#modData = modData.withColumn(\"trip_start_month\", F.col(\"trip_start_month\").cast(\"string\"))\n","#modData = modData.withColumn(\"trip_start_dow\", F.col(\"trip_start_dow\").cast(\"string\"))\n","\n","# drop unneeded columns                                              \n","modData = modData.drop('trip_end_timestamp',\n","                       'pickup_community_area', 'dropoff_community_area', #numeric codes\n","                       'pickup_community_eventCnt','dropoff_community_eventCnt', #keeping combined count\n","                       'chicago_pickup','chicago_dropoff', #keeping combined count\n","                       'PRCP','SNOW' #keeping binary 'rain_show' variable\n","                      )\n","modData.dtypes"]},{"cell_type":"code","execution_count":19,"id":"e811bb49","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/11/26 07:00:05 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n","22/11/26 07:00:08 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n","22/11/26 07:00:09 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n","22/11/26 07:00:09 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n","22/11/26 07:00:09 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n","22/11/26 07:00:09 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n","22/11/26 07:00:09 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n","22/11/26 07:00:09 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["done\n"]}],"source":["# this is the final data, use this\n","modData.write.parquet('gs://big-data-final/model-data/final-model-with-feature.parquet')\n","print('done')"]},{"cell_type":"code","execution_count":23,"id":"4f147b1f","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["220167785"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["#modData.count() #221,302,885, this is #220,167,785"]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":5}